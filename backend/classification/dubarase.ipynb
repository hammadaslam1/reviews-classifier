{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "# gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preparing the data\n",
    "\n",
    "def data_length(file):\n",
    "    # global jsonLength\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # jsonLength = len(data)\n",
    "    return len(data)\n",
    "\n",
    "\n",
    "def load_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# print(data_length('./backend/datasets/combined_data.json'))\n",
    "length = data_length('C:/Hammad Aslam/BS IT (post ADP)/3rd Semester/Capstone Project/Project/backend/datasets/categories/allFiles/computers_laptops.json')\n",
    "data_array = []\n",
    "for i in range(0, length):\n",
    "    data = load_data(\n",
    "        'C:/Hammad Aslam/BS IT (post ADP)/3rd Semester/Capstone Project/Project/backend/datasets/categories/allFiles/computers_Laptops.json')[i][\"reviews\"]\n",
    "    # print(data[0][\"review_body\"])\n",
    "    # print(data[i][\"review_body\"])\n",
    "    for j in range(0, len(data)):\n",
    "        data_array.append(data[j][\"review_body\"])\n",
    "\n",
    "# print(data_array)\n",
    "\n",
    "# stopwords = stopwords.words('english')\n",
    "# print (stopwords)\n",
    "\n",
    "# lemmatization\n",
    "len(data_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    new_record = re.sub(' +', ' ', text)\n",
    "    # print(new_record)\n",
    "    doc = nlp(new_record)\n",
    "    # print(doc)\n",
    "    newText = []\n",
    "    for token in doc:\n",
    "        # if token.pos_ in allowed_postags:\n",
    "        newText.append(token.lemma_)\n",
    "        # token_list = [token for token in doc]\n",
    "        filtered_list = [token for token in doc if not token.is_stop]\n",
    "        lemmas = [\n",
    "            f\"{token.lemma_}\"\n",
    "            for token in filtered_list\n",
    "        ]\n",
    "        # string = ' '.join(lemmas)\n",
    "    final = \" \".join(lemmas)\n",
    "    print(final)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x000002B3972E09A0&gt;)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x000002B3972E09A0&gt;)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x000002B3972E09A0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(preprocessor=<function preprocess_text at 0x000002B3972E09A0>)),\n",
       "                ('clf', LogisticRegression())])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=2, random_state=0)\n",
    "\n",
    "# Define the classifier pipeline\n",
    "classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great opportunity .\n",
      "thankful stable job .\n",
      "happy unstable situation .\n",
      "good sign .\n",
      "Accuracy: 0.5\n",
      "buy Compaq A1500 week satisfied . simple installation problem , excellent photo quality resolution plain paper , good managing software good , need customer support run :) . \n",
      " good product excellent feature , quality resolution . believe well well available price range .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Example usage to filter out words\n",
    "example_text = \"I bought this Compaq A1500 this week and am more than satisfied with it. Simple installation without any problems, excellent photo quality resolution even on plain paper, very good managing software and the best part is, I didn't need any customer support to make it run :).\\nVery good product with excellent features, quality and resolution. Believe me it's better than the best available in this price range.\"\n",
    "predicted_label = classifier.predict([example_text])\n",
    "accuracy\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy Compaq A1500 week satisfied . simple installation problem , excellent photo quality resolution plain paper , good managing software good , need customer support run :) . \n",
      " good product excellent feature , quality resolution . believe well well available price range .\n",
      "Filtered text: buy Compaq A1500 week satisfied . simple installation problem , excellent photo quality resolution plain paper , good managing software good , need customer support run :) . \n",
      " good product excellent feature , quality resolution . believe well well available price range .\n"
     ]
    }
   ],
   "source": [
    "if predicted_label == 1:\n",
    "    print(\"Filtered text:\", preprocess_text(example_text))\n",
    "else:\n",
    "    print(\"Text passed through:\", preprocess_text(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy Compaq A1500 week satisfied . simple installation problem , excellent photo quality resolution plain paper , good managing software good , need customer support run :) . \n",
      " good product excellent feature , quality resolution . believe well well available price range .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WordList(['compaq week', 'simple installation problem', 'excellent photo quality resolution', 'plain paper good', 'software good need customer support run good product', 'quality resolution', 'available price range'])]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newText = []\n",
    "lemText = preprocess_text(example_text)\n",
    "genText = gen_words(lemText)\n",
    "newText.append(genText)\n",
    "newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compaq week', 'simple installation problem', 'excellent photo quality resolution', 'plain paper good', 'software good need customer support run good product', 'quality resolution', 'available price range']\n"
     ]
    }
   ],
   "source": [
    "# BIGRAMS AND TRIGRAMS\n",
    "bigram = gensim.models.Phrases(newText, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[newText], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "bigram_mod\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[newText[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords and lemmatizing\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['compaq', 'week', 'simple', 'installation', 'problem', 'excellent', 'photo', 'quality', 'resolution', 'plain', 'paper', 'good', 'software', 'good', 'need', 'customer', 'support', 'run', 'good', 'product', 'quality', 'resolution', 'available', 'price', 'range']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(newText)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# data_lemmatized\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<21 unique tokens: ['available', 'compaq', 'customer', 'excellent', 'good']...>\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "corpus = []\n",
    "# for text in newText:\n",
    "#     a = id2word.doc2bow(text)\n",
    "#     corpus.append(a)\n",
    "texts = data_lemmatized\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# print(corpus)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('available', 1),\n",
       "  ('compaq', 1),\n",
       "  ('customer', 1),\n",
       "  ('excellent', 1),\n",
       "  ('good', 3),\n",
       "  ('installation', 1),\n",
       "  ('need', 1),\n",
       "  ('paper', 1),\n",
       "  ('photo', 1),\n",
       "  ('plain', 1),\n",
       "  ('price', 1),\n",
       "  ('problem', 1),\n",
       "  ('product', 1),\n",
       "  ('quality', 2),\n",
       "  ('range', 1),\n",
       "  ('resolution', 2),\n",
       "  ('run', 1),\n",
       "  ('simple', 1),\n",
       "  ('software', 1),\n",
       "  ('support', 1),\n",
       "  ('week', 1)]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x2b38d6f9190>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=50,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.048*\"quality\" + 0.048*\"good\" + 0.048*\"resolution\" + 0.048*\"simple\" + '\n",
      "  '0.048*\"run\" + 0.048*\"photo\" + 0.048*\"software\" + 0.048*\"support\" + '\n",
      "  '0.048*\"excellent\" + 0.048*\"product\"'),\n",
      " (1,\n",
      "  '0.048*\"quality\" + 0.048*\"support\" + 0.048*\"resolution\" + 0.048*\"good\" + '\n",
      "  '0.048*\"run\" + 0.048*\"software\" + 0.048*\"need\" + 0.048*\"excellent\" + '\n",
      "  '0.048*\"problem\" + 0.048*\"range\"'),\n",
      " (2,\n",
      "  '0.048*\"good\" + 0.048*\"quality\" + 0.048*\"resolution\" + 0.048*\"photo\" + '\n",
      "  '0.048*\"problem\" + 0.048*\"run\" + 0.048*\"software\" + 0.048*\"simple\" + '\n",
      "  '0.048*\"excellent\" + 0.048*\"support\"'),\n",
      " (3,\n",
      "  '0.050*\"good\" + 0.049*\"resolution\" + 0.048*\"quality\" + 0.048*\"simple\" + '\n",
      "  '0.048*\"support\" + 0.048*\"software\" + 0.048*\"run\" + 0.048*\"installation\" + '\n",
      "  '0.048*\"photo\" + 0.047*\"price\"'),\n",
      " (4,\n",
      "  '0.048*\"good\" + 0.048*\"resolution\" + 0.048*\"quality\" + 0.048*\"photo\" + '\n",
      "  '0.048*\"problem\" + 0.048*\"run\" + 0.048*\"need\" + 0.048*\"installation\" + '\n",
      "  '0.048*\"support\" + 0.048*\"available\"'),\n",
      " (5,\n",
      "  '0.048*\"good\" + 0.048*\"quality\" + 0.048*\"resolution\" + 0.048*\"photo\" + '\n",
      "  '0.048*\"software\" + 0.048*\"problem\" + 0.048*\"simple\" + 0.048*\"excellent\" + '\n",
      "  '0.048*\"installation\" + 0.048*\"need\"'),\n",
      " (6,\n",
      "  '0.048*\"quality\" + 0.048*\"good\" + 0.048*\"software\" + 0.048*\"resolution\" + '\n",
      "  '0.048*\"run\" + 0.048*\"problem\" + 0.048*\"range\" + 0.048*\"need\" + '\n",
      "  '0.048*\"support\" + 0.048*\"installation\"'),\n",
      " (7,\n",
      "  '0.048*\"resolution\" + 0.048*\"good\" + 0.048*\"quality\" + 0.048*\"simple\" + '\n",
      "  '0.048*\"problem\" + 0.048*\"photo\" + 0.048*\"need\" + 0.048*\"software\" + '\n",
      "  '0.048*\"range\" + 0.048*\"installation\"'),\n",
      " (8,\n",
      "  '0.114*\"good\" + 0.078*\"quality\" + 0.077*\"resolution\" + 0.041*\"available\" + '\n",
      "  '0.041*\"excellent\" + 0.041*\"need\" + 0.041*\"week\" + 0.041*\"compaq\" + '\n",
      "  '0.041*\"problem\" + 0.041*\"product\"'),\n",
      " (9,\n",
      "  '0.048*\"quality\" + 0.048*\"good\" + 0.048*\"run\" + 0.048*\"range\" + '\n",
      "  '0.048*\"resolution\" + 0.048*\"software\" + 0.048*\"need\" + 0.048*\"customer\" + '\n",
      "  '0.048*\"support\" + 0.048*\"problem\"')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.048*\"quality\" + 0.048*\"good\" + 0.048*\"resolution\" + 0.048*\"simple\" + 0.048*\"run\" + 0.048*\"photo\" + 0.048*\"software\" + 0.048*\"support\" + 0.048*\"excellent\" + 0.048*\"product\"'),\n",
       " (1,\n",
       "  '0.048*\"quality\" + 0.048*\"support\" + 0.048*\"resolution\" + 0.048*\"good\" + 0.048*\"run\" + 0.048*\"software\" + 0.048*\"need\" + 0.048*\"excellent\" + 0.048*\"problem\" + 0.048*\"range\"'),\n",
       " (2,\n",
       "  '0.048*\"good\" + 0.048*\"quality\" + 0.048*\"resolution\" + 0.048*\"photo\" + 0.048*\"problem\" + 0.048*\"run\" + 0.048*\"software\" + 0.048*\"simple\" + 0.048*\"excellent\" + 0.048*\"support\"'),\n",
       " (3,\n",
       "  '0.050*\"good\" + 0.049*\"resolution\" + 0.048*\"quality\" + 0.048*\"simple\" + 0.048*\"support\" + 0.048*\"software\" + 0.048*\"run\" + 0.048*\"installation\" + 0.048*\"photo\" + 0.047*\"price\"'),\n",
       " (4,\n",
       "  '0.048*\"good\" + 0.048*\"resolution\" + 0.048*\"quality\" + 0.048*\"photo\" + 0.048*\"problem\" + 0.048*\"run\" + 0.048*\"need\" + 0.048*\"installation\" + 0.048*\"support\" + 0.048*\"available\"'),\n",
       " (5,\n",
       "  '0.048*\"good\" + 0.048*\"quality\" + 0.048*\"resolution\" + 0.048*\"photo\" + 0.048*\"software\" + 0.048*\"problem\" + 0.048*\"simple\" + 0.048*\"excellent\" + 0.048*\"installation\" + 0.048*\"need\"'),\n",
       " (6,\n",
       "  '0.048*\"quality\" + 0.048*\"good\" + 0.048*\"software\" + 0.048*\"resolution\" + 0.048*\"run\" + 0.048*\"problem\" + 0.048*\"range\" + 0.048*\"need\" + 0.048*\"support\" + 0.048*\"installation\"'),\n",
       " (7,\n",
       "  '0.048*\"resolution\" + 0.048*\"good\" + 0.048*\"quality\" + 0.048*\"simple\" + 0.048*\"problem\" + 0.048*\"photo\" + 0.048*\"need\" + 0.048*\"software\" + 0.048*\"range\" + 0.048*\"installation\"'),\n",
       " (8,\n",
       "  '0.114*\"good\" + 0.078*\"quality\" + 0.077*\"resolution\" + 0.041*\"available\" + 0.041*\"excellent\" + 0.041*\"need\" + 0.041*\"week\" + 0.041*\"compaq\" + 0.041*\"problem\" + 0.041*\"product\"'),\n",
       " (9,\n",
       "  '0.048*\"quality\" + 0.048*\"good\" + 0.048*\"run\" + 0.048*\"range\" + 0.048*\"resolution\" + 0.048*\"software\" + 0.048*\"need\" + 0.048*\"customer\" + 0.048*\"support\" + 0.048*\"problem\"')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "lda_model.print_topics()\n",
    "# doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -4.390558569431305\n",
      "\n",
      "Coherence Score:  0.9999999999999998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.coherencemodel.CoherenceModel at 0x2b398e6a3f0>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "coherence_model_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyLDAvis\\_display.py:308\u001b[0m, in \u001b[0;36menable_notebook.<locals>.<lambda>\u001b[1;34m(data, kwds)\u001b[0m\n\u001b[0;32m    305\u001b[0m ip \u001b[38;5;241m=\u001b[39m get_ipython()\n\u001b[0;32m    306\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ip\u001b[38;5;241m.\u001b[39mdisplay_formatter\u001b[38;5;241m.\u001b[39mformatters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/html\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    307\u001b[0m formatter\u001b[38;5;241m.\u001b[39mfor_type(PreparedData,\n\u001b[1;32m--> 308\u001b[0m                    \u001b[38;5;28;01mlambda\u001b[39;00m data, kwds\u001b[38;5;241m=\u001b[39mkwargs: \u001b[43mprepared_data_to_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyLDAvis\\_display.py:177\u001b[0m, in \u001b[0;36mprepared_data_to_html\u001b[1;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, visid):\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisid must not contain spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m template\u001b[38;5;241m.\u001b[39mrender(visid\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(visid),\n\u001b[0;32m    174\u001b[0m                        visid_raw\u001b[38;5;241m=\u001b[39mvisid,\n\u001b[0;32m    175\u001b[0m                        d3_url\u001b[38;5;241m=\u001b[39md3_url,\n\u001b[0;32m    176\u001b[0m                        ldavis_url\u001b[38;5;241m=\u001b[39mldavis_url,\n\u001b[1;32m--> 177\u001b[0m                        vis_json\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    178\u001b[0m                        ldavis_css_url\u001b[38;5;241m=\u001b[39mldavis_css_url)\n",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyLDAvis\\_prepare.py:464\u001b[0m, in \u001b[0;36mPreparedData.to_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_json\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNumPyEncoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyLDAvis\\utils.py:150\u001b[0m, in \u001b[0;36mNumPyEncoder.default\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mfloat64) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(obj)\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJSONEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=                        x         y  topics  cluster       Freq\n",
       "topic                                                          \n",
       "8     -0.011792+0.000000j  0.0-0.0j       1        1  98.827474\n",
       "3      0.000536+0.000000j  0.0-0.0j       2        1   0.181469\n",
       "0      0.001407+0.000000j  0.0-0.0j       3        1   0.123883\n",
       "5      0.001407+0.000000j  0.0+0.0j       4        1   0.123883\n",
       "4      0.001407+0.000000j -0.0+0.0j       5        1   0.123882\n",
       "6      0.001407+0.000000j  0.0+0.0j       6        1   0.123882\n",
       "7      0.001407+0.000000j  0.0-0.0j       7        1   0.123882\n",
       "1      0.001407+0.000000j  0.0-0.0j       8        1   0.123882\n",
       "9      0.001407+0.000000j -0.0+0.0j       9        1   0.123882\n",
       "2      0.001407+0.000000j -0.0+0.0j      10        1   0.123881, topic_info=          Term      Freq     Total Category  logprob  loglift\n",
       "4         good  2.000000  2.000000  Default  21.0000  21.0000\n",
       "13     quality  1.000000  1.000000  Default  20.0000  20.0000\n",
       "15  resolution  1.000000  1.000000  Default  19.0000  19.0000\n",
       "0    available  1.000000  1.000000  Default  18.0000  18.0000\n",
       "3    excellent  1.000000  1.000000  Default  17.0000  17.0000\n",
       "..         ...       ...       ...      ...      ...      ...\n",
       "3    excellent  0.001475  1.017319  Topic10  -3.0445   0.1572\n",
       "0    available  0.001475  1.017548  Topic10  -3.0445   0.1570\n",
       "15  resolution  0.001475  1.927968  Topic10  -3.0445  -0.4821\n",
       "13     quality  0.001475  1.928849  Topic10  -3.0445  -0.4826\n",
       "4         good  0.001475  2.838926  Topic10  -3.0445  -0.8691\n",
       "\n",
       "[231 rows x 6 columns], token_table=      Topic      Freq          Term\n",
       "term                               \n",
       "0         1  0.982755     available\n",
       "1         1  0.983180        compaq\n",
       "2         1  0.983302      customer\n",
       "3         1  0.982976     excellent\n",
       "4         1  1.056738          good\n",
       "5         1  0.983675  installation\n",
       "6         1  0.983100          need\n",
       "7         1  0.983389         paper\n",
       "8         1  0.983612         photo\n",
       "9         1  0.983427         plain\n",
       "10        1  0.983489         price\n",
       "11        1  0.983197       problem\n",
       "12        1  0.983210       product\n",
       "13        1  1.036888       quality\n",
       "14        1  0.983424         range\n",
       "15        1  1.037362    resolution\n",
       "16        1  0.983693           run\n",
       "17        1  0.983788        simple\n",
       "18        1  0.983700      software\n",
       "19        1  0.983752       support\n",
       "20        1  0.983132          week, R=21, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[9, 4, 1, 6, 5, 7, 8, 2, 10, 3])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
