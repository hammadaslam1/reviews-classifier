{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"C:/Hammad Aslam/BS IT (post ADP)/3rd Semester/Capstone Project/reviews-classifier/backend/practice/afterEval/sample_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Neg_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Unique_words</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Adj_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Adv_Count</th>\n",
       "      <th>Pro_Count</th>\n",
       "      <th>Pre_Count</th>\n",
       "      <th>Con_Count</th>\n",
       "      <th>Art_Count</th>\n",
       "      <th>Aux_Count</th>\n",
       "      <th>Authenticity</th>\n",
       "      <th>AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>0.473750</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating label                                               text  Sentiment  \\\n",
       "0       5    CG  Love this!  Well made, sturdy, and very comfor...   0.473750   \n",
       "1       5    CG  love it, a great upgrade from the original.  I...   0.558333   \n",
       "2       5    CG  This pillow saved my back. I love the look and...   0.250000   \n",
       "3       1    CG  Missing information on how to use it, but it i...   0.400000   \n",
       "4       5    CG  Very nice set. Good quality. We have had the s...   0.740000   \n",
       "\n",
       "   Subjectivity  Neg_Count  Word_Count  Unique_words  Noun_Count  Adj_Count  \\\n",
       "0           0.8          0          12            11         5.0        1.0   \n",
       "1           0.7          0          17            16         5.0        2.0   \n",
       "2           0.3          0          14            13         5.0        0.0   \n",
       "3           0.4          1          17            17         4.0        1.0   \n",
       "4           0.8          0          18            17         3.0        2.0   \n",
       "\n",
       "   Verb_Count  Adv_Count  Pro_Count  Pre_Count  Con_Count  Art_Count  \\\n",
       "0         2.0        1.0        0.0        0.0        1.0        0.0   \n",
       "1         3.0        0.0        1.0        3.0        0.0        2.0   \n",
       "2         2.0        0.0        1.0        1.0        1.0        1.0   \n",
       "3         3.0        1.0        1.0        2.0        1.0        2.0   \n",
       "4         3.0        3.0        1.0        1.0        1.0        1.0   \n",
       "\n",
       "   Aux_Count  Authenticity    AT  \n",
       "0        0.0      0.916667  28.0  \n",
       "1        2.0      1.000000  32.0  \n",
       "2        0.0      1.000000  30.0  \n",
       "3        1.0      1.000000  30.0  \n",
       "4        3.0      1.000000  23.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Neg_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Unique_words</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Adj_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Adv_Count</th>\n",
       "      <th>Pro_Count</th>\n",
       "      <th>Pre_Count</th>\n",
       "      <th>Con_Count</th>\n",
       "      <th>Art_Count</th>\n",
       "      <th>Aux_Count</th>\n",
       "      <th>Authenticity</th>\n",
       "      <th>AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>0.473750</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  label                                               text  \\\n",
       "0       5      0  Love this!  Well made, sturdy, and very comfor...   \n",
       "1       5      0  love it, a great upgrade from the original.  I...   \n",
       "2       5      0  This pillow saved my back. I love the look and...   \n",
       "3       1      0  Missing information on how to use it, but it i...   \n",
       "4       5      0  Very nice set. Good quality. We have had the s...   \n",
       "\n",
       "   Sentiment  Subjectivity  Neg_Count  Word_Count  Unique_words  Noun_Count  \\\n",
       "0   0.473750           0.8          0          12            11         5.0   \n",
       "1   0.558333           0.7          0          17            16         5.0   \n",
       "2   0.250000           0.3          0          14            13         5.0   \n",
       "3   0.400000           0.4          1          17            17         4.0   \n",
       "4   0.740000           0.8          0          18            17         3.0   \n",
       "\n",
       "   Adj_Count  Verb_Count  Adv_Count  Pro_Count  Pre_Count  Con_Count  \\\n",
       "0        1.0         2.0        1.0        0.0        0.0        1.0   \n",
       "1        2.0         3.0        0.0        1.0        3.0        0.0   \n",
       "2        0.0         2.0        0.0        1.0        1.0        1.0   \n",
       "3        1.0         3.0        1.0        1.0        2.0        1.0   \n",
       "4        2.0         3.0        3.0        1.0        1.0        1.0   \n",
       "\n",
       "   Art_Count  Aux_Count  Authenticity    AT  \n",
       "0        0.0        0.0      0.916667  28.0  \n",
       "1        2.0        2.0      1.000000  32.0  \n",
       "2        1.0        0.0      1.000000  30.0  \n",
       "3        2.0        1.0      1.000000  30.0  \n",
       "4        1.0        3.0      1.000000  23.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['label'] = le.fit_transform(data['label'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Neg_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Unique_words</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Adj_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Adv_Count</th>\n",
       "      <th>Pro_Count</th>\n",
       "      <th>Pre_Count</th>\n",
       "      <th>Con_Count</th>\n",
       "      <th>Art_Count</th>\n",
       "      <th>Aux_Count</th>\n",
       "      <th>Authenticity</th>\n",
       "      <th>AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.736875</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029491</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707244</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042895</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.042895</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045576</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.696429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  Sentiment  Subjectivity  Neg_Count  Word_Count  Unique_words  \\\n",
       "0     1.0   0.736875           0.8   0.000000    0.029491      0.041667   \n",
       "1     1.0   0.779167           0.7   0.000000    0.042895      0.062500   \n",
       "2     1.0   0.625000           0.3   0.000000    0.034853      0.050000   \n",
       "3     0.0   0.700000           0.4   0.030303    0.042895      0.066667   \n",
       "4     1.0   0.870000           0.8   0.000000    0.045576      0.066667   \n",
       "\n",
       "   Noun_Count  Adj_Count  Verb_Count  Adv_Count  Pro_Count  Pre_Count  \\\n",
       "0    0.048077   0.021277    0.033898   0.029412   0.000000   0.000000   \n",
       "1    0.048077   0.042553    0.050847   0.000000   0.055556   0.125000   \n",
       "2    0.048077   0.000000    0.033898   0.000000   0.055556   0.041667   \n",
       "3    0.038462   0.021277    0.050847   0.029412   0.055556   0.083333   \n",
       "4    0.028846   0.042553    0.050847   0.088235   0.055556   0.041667   \n",
       "\n",
       "   Con_Count  Art_Count  Aux_Count  Authenticity        AT  \n",
       "0      0.125   0.000000   0.000000      0.707244  0.785714  \n",
       "1      0.000   0.153846   0.111111      0.754902  0.857143  \n",
       "2      0.125   0.076923   0.000000      0.754902  0.821429  \n",
       "3      0.125   0.153846   0.055556      0.754902  0.821429  \n",
       "4      0.125   0.076923   0.166667      0.754902  0.696429  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "# y_scaler = MinMaxScaler()\n",
    "y = data['label']\n",
    "data = data.drop(['label', 'text'], axis=1)\n",
    "Columns=data.columns\n",
    "# Columns\n",
    "data[Columns] = min_max_scaler.fit_transform(data[Columns])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Neg_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Unique_words</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Adj_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Adv_Count</th>\n",
       "      <th>Pro_Count</th>\n",
       "      <th>Pre_Count</th>\n",
       "      <th>Con_Count</th>\n",
       "      <th>Art_Count</th>\n",
       "      <th>Aux_Count</th>\n",
       "      <th>Authenticity</th>\n",
       "      <th>AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40422</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.646116</td>\n",
       "      <td>0.623113</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.772118</td>\n",
       "      <td>0.554167</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.444218</td>\n",
       "      <td>0.160714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40423</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.608023</td>\n",
       "      <td>0.504678</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.935657</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.259615</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516340</td>\n",
       "      <td>0.482143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40424</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.628471</td>\n",
       "      <td>0.617282</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.788204</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.567308</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.417581</td>\n",
       "      <td>0.482143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40425</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.584204</td>\n",
       "      <td>0.466054</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.924933</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.520193</td>\n",
       "      <td>0.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40426</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.716584</td>\n",
       "      <td>0.679799</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.707775</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.441978</td>\n",
       "      <td>0.321429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.595216</td>\n",
       "      <td>0.486501</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.900804</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.495258</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.639489</td>\n",
       "      <td>0.652157</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.729223</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.426010</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.527320</td>\n",
       "      <td>0.405525</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.938338</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.530054</td>\n",
       "      <td>0.446429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.649499</td>\n",
       "      <td>0.618836</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.782842</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.432692</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.409422</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.567970</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.943700</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471384</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating  Sentiment  Subjectivity  Neg_Count  Word_Count  Unique_words  \\\n",
       "40422    0.75   0.646116      0.623113   0.333333    0.772118      0.554167   \n",
       "40423    0.75   0.608023      0.504678   0.121212    0.935657      0.833333   \n",
       "40424    0.50   0.628471      0.617282   0.484848    0.788204      0.537500   \n",
       "40425    1.00   0.584204      0.466054   0.090909    0.924933      0.816667   \n",
       "40426    1.00   0.716584      0.679799   0.151515    0.707775      0.491667   \n",
       "40427    0.75   0.595216      0.486501   0.212121    0.900804      0.770833   \n",
       "40428    1.00   0.639489      0.652157   0.303030    0.729223      0.487500   \n",
       "40429    0.25   0.527320      0.405525   0.333333    0.938338      0.912500   \n",
       "40430    0.00   0.649499      0.618836   0.303030    0.782842      0.491667   \n",
       "40431    1.00   0.567970      0.541339   0.242424    0.943700      0.770833   \n",
       "\n",
       "       Noun_Count  Adj_Count  Verb_Count  Adv_Count  Pro_Count  Pre_Count  \\\n",
       "40422    0.480769   0.531915    0.779661   0.764706   0.500000   0.583333   \n",
       "40423    0.259615   0.553191    0.508475   0.352941   0.388889   0.416667   \n",
       "40424    0.567308   0.595745    0.745763   0.411765   0.388889   0.750000   \n",
       "40425    0.278846   0.361702    0.406780   0.205882   0.555556   0.375000   \n",
       "40426    0.403846   0.574468    0.661017   0.558824   0.333333   0.625000   \n",
       "40427    0.230769   0.446809    0.372881   0.235294   0.277778   0.333333   \n",
       "40428    0.807692   0.617021    0.762712   0.382353   0.444444   0.666667   \n",
       "40429    0.201923   0.489362    0.271186   0.441176   0.222222   0.375000   \n",
       "40430    0.432692   0.510638    0.593220   0.529412   0.388889   0.708333   \n",
       "40431    0.000000   0.000000    0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "       Con_Count  Art_Count  Aux_Count  Authenticity        AT  \n",
       "40422      0.500   0.384615   0.833333      0.444218  0.160714  \n",
       "40423      0.250   0.153846   0.500000      0.516340  0.482143  \n",
       "40424      0.375   0.230769   0.777778      0.417581  0.482143  \n",
       "40425      0.250   0.153846   0.611111      0.520193  0.464286  \n",
       "40426      0.375   0.153846   0.833333      0.441978  0.321429  \n",
       "40427      0.250   0.153846   0.444444      0.495258  0.571429  \n",
       "40428      0.375   0.153846   0.555556      0.426010  0.500000  \n",
       "40429      0.375   0.153846   0.444444      0.530054  0.446429  \n",
       "40430      0.250   0.153846   0.833333      0.409422  0.357143  \n",
       "40431      0.000   0.000000   0.000000      0.471384  0.821429  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Neg_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Unique_words</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Adj_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Adv_Count</th>\n",
       "      <th>Pro_Count</th>\n",
       "      <th>Pre_Count</th>\n",
       "      <th>Con_Count</th>\n",
       "      <th>Art_Count</th>\n",
       "      <th>Aux_Count</th>\n",
       "      <th>Authenticity</th>\n",
       "      <th>AT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.736875</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029491</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707244</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042895</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.042895</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045576</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.696429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  Sentiment  Subjectivity  Neg_Count  Word_Count  Unique_words  \\\n",
       "0     1.0   0.736875           0.8   0.000000    0.029491      0.041667   \n",
       "1     1.0   0.779167           0.7   0.000000    0.042895      0.062500   \n",
       "2     1.0   0.625000           0.3   0.000000    0.034853      0.050000   \n",
       "3     0.0   0.700000           0.4   0.030303    0.042895      0.066667   \n",
       "4     1.0   0.870000           0.8   0.000000    0.045576      0.066667   \n",
       "\n",
       "   Noun_Count  Adj_Count  Verb_Count  Adv_Count  Pro_Count  Pre_Count  \\\n",
       "0    0.048077   0.021277    0.033898   0.029412   0.000000   0.000000   \n",
       "1    0.048077   0.042553    0.050847   0.000000   0.055556   0.125000   \n",
       "2    0.048077   0.000000    0.033898   0.000000   0.055556   0.041667   \n",
       "3    0.038462   0.021277    0.050847   0.029412   0.055556   0.083333   \n",
       "4    0.028846   0.042553    0.050847   0.088235   0.055556   0.041667   \n",
       "\n",
       "   Con_Count  Art_Count  Aux_Count  Authenticity        AT  \n",
       "0      0.125   0.000000   0.000000      0.707244  0.785714  \n",
       "1      0.000   0.153846   0.111111      0.754902  0.857143  \n",
       "2      0.125   0.076923   0.000000      0.754902  0.821429  \n",
       "3      0.125   0.153846   0.055556      0.754902  0.821429  \n",
       "4      0.125   0.076923   0.166667      0.754902  0.696429  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Use 'categorical_crossentropy' for multi-class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8426 - loss: 0.3397 - val_accuracy: 0.8338 - val_loss: 0.3473\n",
      "Epoch 2/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8495 - loss: 0.3268 - val_accuracy: 0.8172 - val_loss: 0.3855\n",
      "Epoch 3/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8426 - loss: 0.3388 - val_accuracy: 0.8328 - val_loss: 0.3521\n",
      "Epoch 4/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8472 - loss: 0.3333 - val_accuracy: 0.8374 - val_loss: 0.3488\n",
      "Epoch 5/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8482 - loss: 0.3283 - val_accuracy: 0.8354 - val_loss: 0.3500\n",
      "Epoch 6/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.8436 - loss: 0.3301 - val_accuracy: 0.8384 - val_loss: 0.3463\n",
      "Epoch 7/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8479 - loss: 0.3316 - val_accuracy: 0.8402 - val_loss: 0.3411\n",
      "Epoch 8/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8400 - loss: 0.3396 - val_accuracy: 0.8325 - val_loss: 0.3539\n",
      "Epoch 9/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8460 - loss: 0.3336 - val_accuracy: 0.8405 - val_loss: 0.3417\n",
      "Epoch 10/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8429 - loss: 0.3350 - val_accuracy: 0.8430 - val_loss: 0.3406\n",
      "Epoch 11/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8485 - loss: 0.3274 - val_accuracy: 0.8348 - val_loss: 0.3596\n",
      "Epoch 12/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8423 - loss: 0.3406 - val_accuracy: 0.8381 - val_loss: 0.3434\n",
      "Epoch 13/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8447 - loss: 0.3326 - val_accuracy: 0.8404 - val_loss: 0.3433\n",
      "Epoch 14/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8497 - loss: 0.3289 - val_accuracy: 0.8371 - val_loss: 0.3561\n",
      "Epoch 15/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8464 - loss: 0.3284 - val_accuracy: 0.8398 - val_loss: 0.3430\n",
      "Epoch 16/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8454 - loss: 0.3350 - val_accuracy: 0.8375 - val_loss: 0.3437\n",
      "Epoch 17/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8490 - loss: 0.3231 - val_accuracy: 0.8435 - val_loss: 0.3402\n",
      "Epoch 18/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8448 - loss: 0.3290 - val_accuracy: 0.8367 - val_loss: 0.3413\n",
      "Epoch 19/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8520 - loss: 0.3247 - val_accuracy: 0.8323 - val_loss: 0.3566\n",
      "Epoch 20/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8498 - loss: 0.3297 - val_accuracy: 0.8336 - val_loss: 0.3565\n",
      "Epoch 21/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8485 - loss: 0.3289 - val_accuracy: 0.8383 - val_loss: 0.3374\n",
      "Epoch 22/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8492 - loss: 0.3247 - val_accuracy: 0.8396 - val_loss: 0.3445\n",
      "Epoch 23/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8474 - loss: 0.3285 - val_accuracy: 0.8407 - val_loss: 0.3431\n",
      "Epoch 24/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8424 - loss: 0.3314 - val_accuracy: 0.8389 - val_loss: 0.3419\n",
      "Epoch 25/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8524 - loss: 0.3237 - val_accuracy: 0.8401 - val_loss: 0.3358\n",
      "Epoch 26/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8498 - loss: 0.3224 - val_accuracy: 0.8456 - val_loss: 0.3349\n",
      "Epoch 27/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8469 - loss: 0.3305 - val_accuracy: 0.8423 - val_loss: 0.3420\n",
      "Epoch 28/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8486 - loss: 0.3256 - val_accuracy: 0.8253 - val_loss: 0.3725\n",
      "Epoch 29/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8516 - loss: 0.3229 - val_accuracy: 0.8429 - val_loss: 0.3404\n",
      "Epoch 30/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.8490 - loss: 0.3276 - val_accuracy: 0.8410 - val_loss: 0.3349\n",
      "Epoch 31/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8514 - loss: 0.3239 - val_accuracy: 0.8381 - val_loss: 0.3491\n",
      "Epoch 32/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8531 - loss: 0.3215 - val_accuracy: 0.8375 - val_loss: 0.3427\n",
      "Epoch 33/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8497 - loss: 0.3225 - val_accuracy: 0.8407 - val_loss: 0.3368\n",
      "Epoch 34/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8534 - loss: 0.3164 - val_accuracy: 0.8335 - val_loss: 0.3540\n",
      "Epoch 35/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8526 - loss: 0.3200 - val_accuracy: 0.8396 - val_loss: 0.3404\n",
      "Epoch 36/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8472 - loss: 0.3241 - val_accuracy: 0.8402 - val_loss: 0.3388\n",
      "Epoch 37/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8533 - loss: 0.3180 - val_accuracy: 0.8349 - val_loss: 0.3515\n",
      "Epoch 38/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.8528 - loss: 0.3186 - val_accuracy: 0.8414 - val_loss: 0.3345\n",
      "Epoch 39/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8527 - loss: 0.3203 - val_accuracy: 0.8388 - val_loss: 0.3435\n",
      "Epoch 40/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8534 - loss: 0.3226 - val_accuracy: 0.8414 - val_loss: 0.3358\n",
      "Epoch 41/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8478 - loss: 0.3239 - val_accuracy: 0.8443 - val_loss: 0.3345\n",
      "Epoch 42/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8535 - loss: 0.3181 - val_accuracy: 0.8361 - val_loss: 0.3479\n",
      "Epoch 43/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8491 - loss: 0.3229 - val_accuracy: 0.8408 - val_loss: 0.3468\n",
      "Epoch 44/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8526 - loss: 0.3138 - val_accuracy: 0.8427 - val_loss: 0.3371\n",
      "Epoch 45/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8530 - loss: 0.3197 - val_accuracy: 0.8414 - val_loss: 0.3395\n",
      "Epoch 46/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8549 - loss: 0.3108 - val_accuracy: 0.8423 - val_loss: 0.3404\n",
      "Epoch 47/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8519 - loss: 0.3179 - val_accuracy: 0.8418 - val_loss: 0.3388\n",
      "Epoch 48/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8523 - loss: 0.3169 - val_accuracy: 0.8431 - val_loss: 0.3398\n",
      "Epoch 49/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8522 - loss: 0.3223 - val_accuracy: 0.8443 - val_loss: 0.3368\n",
      "Epoch 50/50\n",
      "\u001b[1m1982/1982\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8527 - loss: 0.3221 - val_accuracy: 0.8190 - val_loss: 0.3932\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function Layer._initialize_tracker.<locals>.<lambda> at 0x000001A35A6F2020>: it's not found as keras.src.layers.layer.Layer._initialize_tracker.<locals>.<lambda>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dump, load\n\u001b[1;32m----> 2\u001b[0m dump(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Hammad Aslam/BS IT (post ADP)/3rd Semester/Capstone Project/reviews-classifier/backend/models/multimodel/FakenessNeuralNetwork.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:553\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     NumpyPickler(filename, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[1;32m--> 487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(obj)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_reduce(obj\u001b[38;5;241m=\u001b[39mobj, \u001b[38;5;241m*\u001b[39mrv)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_setitems(obj\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[0;32m    997\u001b[0m         save(k)\n\u001b[1;32m--> 998\u001b[0m         save(v)\n\u001b[0;32m    999\u001b[0m     write(SETITEMS)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_reduce(obj\u001b[38;5;241m=\u001b[39mobj, \u001b[38;5;241m*\u001b[39mrv)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_setitems(obj\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[0;32m    997\u001b[0m         save(k)\n\u001b[1;32m--> 998\u001b[0m         save(v)\n\u001b[0;32m    999\u001b[0m     write(SETITEMS)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_setitems(obj\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[0;32m    997\u001b[0m         save(k)\n\u001b[1;32m--> 998\u001b[0m         save(v)\n\u001b[0;32m    999\u001b[0m     write(SETITEMS)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[1;32m--> 887\u001b[0m         save(element)\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;66;03m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(obj) \u001b[38;5;129;01min\u001b[39;00m memo:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\pickle.py:1071\u001b[0m, in \u001b[0;36m_Pickler.save_global\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     obj2, parent \u001b[38;5;241m=\u001b[39m _getattribute(module, name)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m-> 1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\n\u001b[0;32m   1072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m: it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not found as \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1073\u001b[0m         (obj, module_name, name)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj:\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function Layer._initialize_tracker.<locals>.<lambda> at 0x000001A35A6F2020>: it's not found as keras.src.layers.layer.Layer._initialize_tracker.<locals>.<lambda>"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'C:/Hammad Aslam/BS IT (post ADP)/3rd Semester/Capstone Project/reviews-classifier/backend/models/multimodel/FakenessNeuralNetwork.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8312 - loss: 0.3825\n",
      "Test Accuracy: 82.54%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
