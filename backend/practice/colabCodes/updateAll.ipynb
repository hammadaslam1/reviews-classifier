{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# updating all dataset files"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[nltk_data] Downloading package vader_lexicon to\n",
                  "[nltk_data]     C:\\Users\\GS\\AppData\\Roaming\\nltk_data...\n",
                  "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
                  "[nltk_data] Downloading package punkt to\n",
                  "[nltk_data]     C:\\Users\\GS\\AppData\\Roaming\\nltk_data...\n",
                  "[nltk_data]   Package punkt is already up-to-date!\n",
                  "[nltk_data] Downloading package stopwords to\n",
                  "[nltk_data]     C:\\Users\\GS\\AppData\\Roaming\\nltk_data...\n",
                  "[nltk_data]   Package stopwords is already up-to-date!\n",
                  "[nltk_data] Downloading package wordnet to\n",
                  "[nltk_data]     C:\\Users\\GS\\AppData\\Roaming\\nltk_data...\n",
                  "[nltk_data]   Package wordnet is already up-to-date!\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 1,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import os\n",
            "# import pymongo\n",
            "import json\n",
            "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
            "from sentence_transformers import SentenceTransformer\n",
            "from sklearn.metrics.pairwise import cosine_similarity\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import nltk\n",
            "from nltk.tokenize import word_tokenize\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.stem import WordNetLemmatizer\n",
            "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
            "# Download nltk resources\n",
            "nltk.download('vader_lexicon')\n",
            "nltk.download('punkt')\n",
            "nltk.download('stopwords')\n",
            "nltk.download('wordnet')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\GS\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
                  "  warnings.warn(\n"
               ]
            }
         ],
         "source": [
            "# Load pre-trained BERT model\n",
            "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### directory path"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "homePath = \"C:/Hammad Aslam/BS IT (post ADP)/3rd Semester/Capstone Project/Project/backend/datasets/categories/allFiles\""
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### preprocessing and calculating the similarity"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Function to preprocess text\n",
            "def preprocess_text(text):\n",
            "    tokens = word_tokenize(text)\n",
            "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
            "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
            "    lemmatizer = WordNetLemmatizer()\n",
            "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
            "    return ' '.join(tokens)\n",
            "\n",
            "# Function to calculate cosine similarity between two sentences\n",
            "def calculate_similarity(sentence1, sentence2):\n",
            "    if not isinstance(sentence1, str) or not isinstance(sentence2, str):\n",
            "        return 0.0  # Return zero\n",
            "    if len(sentence1) == 0 or len(sentence2) == 0:\n",
            "        return 0.0  # Return zero similarity if any sentence is empty\n",
            "\n",
            "    # Preprocess sentences\n",
            "    preprocessed_sentence1 = preprocess_text(sentence1)\n",
            "    preprocessed_sentence2 = preprocess_text(sentence2)\n",
            "\n",
            "    # Encode sentences using BERT model\n",
            "    embedding1 = bert_model.encode(preprocessed_sentence1)\n",
            "    embedding2 = bert_model.encode(preprocessed_sentence2)\n",
            "\n",
            "    # Calculate cosine similarity between embeddings\n",
            "    similarity_score = cosine_similarity([embedding1], [embedding2])[0][0]\n",
            "    return max(similarity_score, 0.0)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### updates the data with sentiment scores and helpfulness scores"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "# global count\n",
            "def update_data(data):\n",
            "    count = 0\n",
            "    for i, product in enumerate(data):\n",
            "        categories_str = ', '.join(product['category'])\n",
            "        for j, review in enumerate(product['reviews']):\n",
            "            # Convert list of categories to string\n",
            "            features_str = review['review_topics']\n",
            "\n",
            "            similarity_text_description = calculate_similarity(review['review_body'], product['product_description'])\n",
            "            similarity_text_features = calculate_similarity(review['review_body'], features_str)\n",
            "\n",
            "            # Ensure categories_str is a string\n",
            "            if isinstance(categories_str, np.ndarray):\n",
            "                categories_str = ', '.join(categories_str)\n",
            "\n",
            "            # Split categories_str into individual categories and calculate similarity for each\n",
            "            similarity_text_categories = 0.0\n",
            "            categories = categories_str.split(', ')\n",
            "            for category in categories:\n",
            "                similarity_text_categories += calculate_similarity(review['review_body'], category)\n",
            "            similarity_text_categories /= len(categories)  # Average similarity over all categories\n",
            "\n",
            "            # similarity_text_details = calculate_similarity(review['review_body'], review['details'])\n",
            "            sia = SentimentIntensityAnalyzer()\n",
            "            sentiment_scores = sia.polarity_scores(review['review_body'] if isinstance(review['review_body'], str) else '')\n",
            "            # Update the DataFrame with similarity scores\n",
            "            review['similarity_text_description'] = similarity_text_description\n",
            "            review['similarity_text_features'] = similarity_text_features\n",
            "            review['similarity_text_categories'] = similarity_text_categories\n",
            "            # review['similarity_text_details'] = similarity_text_details\n",
            "            review['sentiment_scores'] = sentiment_scores['compound']\n",
            "            \n",
            "            # review['review_length'] = len(''.join(review['review_body'].split(' ')))\n",
            "            # review['reviews_count'] = 1\n",
            "            # review['avg_review_length'] = review['review_length']\n",
            "            # review['avg_rating'] = review['review_rating']\n",
            "\n",
            "            count += 1\n",
            "            # print(f\"{count}\")\n",
            "            # print(merged_df3.iloc[i])\n",
            "    return data"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### reads the files and punches the updated data in the file"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "def readFiles(file):\n",
            "    with open(file, \"r\") as f:\n",
            "        data = json.load(f)\n",
            "    \n",
            "    newData = update_data(data)\n",
            "    def convert(obj):\n",
            "        if isinstance(obj, np.float32):\n",
            "            return float(obj)\n",
            "        elif isinstance(obj, list):\n",
            "            return [convert(item) for item in obj]\n",
            "        elif isinstance(obj, dict):\n",
            "            return {key: convert(value) for key, value in obj.items()}\n",
            "        else:\n",
            "            return obj\n",
            "\n",
            "    newData_serializable = convert(newData)\n",
            "    # newData_serializable = json.loads(json.dumps(newData, cls=np.array))\n",
            "    # print(newData)\n",
            "    with open(file, 'w') as json_file:\n",
            "        json.dump(newData_serializable, json_file, indent=4)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### accessing each file exactly once in the directory"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "0 done\n",
                  "52 done\n",
                  "1790 done\n",
                  "2282 done\n",
                  "5510 done\n",
                  "6439 done\n",
                  "8571 done\n",
                  "10532 done\n",
                  "12253 done\n",
                  "12673 done\n",
                  "13389 done\n",
                  "14580 done\n",
                  "done\n"
               ]
            }
         ],
         "source": [
            "import csv\n",
            "import json\n",
            "import os\n",
            "\n",
            "def jsonToCsv(attributes, output_file):\n",
            "    count = 0\n",
            "    # homePath = 'D:/BS-IT/4th semester/Capstone Project II/opinio/reviews-classifier/backend/datasets/categories/allFiles'\n",
            "    with open(output_file, 'w', newline='') as csvfile:\n",
            "        # len(data[28]['reviews'])\n",
            "        for filename in os.listdir(homePath):\n",
            "            if filename.endswith(\".json\"):\n",
            "                file = homePath + \"/\" + filename\n",
            "                with open(file, \"r\") as f:\n",
            "                    data = json.load(f)\n",
            "                # readFiles(file)\n",
            "                print(f\"{count} done\")\n",
            "                for i, product in enumerate(data):\n",
            "                    # for j, review in enumerate(data[28]['reviews']):\n",
            "                    csv_writer = csv.writer(csvfile)\n",
            "                    # Write the header row with selected attributes\n",
            "                    csv_writer.writerow(attributes)\n",
            "                    # Loop through each JSON object (if it's an array)\n",
            "                    if isinstance(product['reviews'], list):\n",
            "                        for item in product['reviews']:\n",
            "                            # if int(item['review_votes']) == 0:\n",
            "                            # Extract values for selected attributes\n",
            "                            row = [item.get(attr) for attr in attributes]\n",
            "                            csv_writer.writerow(row)\n",
            "                            count += 1\n",
            "                    else:\n",
            "                        # if int(item['review_votes']) == 0:\n",
            "                        # Handle single JSON object\n",
            "                        row = [product['reviews'].get(attr) for attr in attributes]\n",
            "                        csv_writer.writerow(row)\n",
            "                        count += 1\n",
            "\n",
            "attributes_to_include = [\"review_body\", \"review_topics\", \"review_votes\", \"sentiment\", \"review_helpfulness\"]\n",
            "output_filename = \"C:/Hammad Aslam/BS IT (post ADP)/3rd Semester/Capstone Project/Project/backend/practice/graph/no_votes.csv\"\n",
            "\n",
            "jsonToCsv(attributes_to_include, output_filename)\n",
            "\n",
            "print('done')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "1 done\n",
                  "2 done\n",
                  "3 done\n",
                  "4 done\n",
                  "5 done\n",
                  "6 done\n",
                  "7 done\n",
                  "8 done\n",
                  "9 done\n",
                  "10 done\n",
                  "11 done\n"
               ]
            }
         ],
         "source": [
            "count1 = 0\n",
            "for filename in os.listdir(homePath):\n",
            "    if filename.endswith(\".json\"):\n",
            "        file = homePath + \"/\" + filename\n",
            "        if filename != \"computers_laptops.json\":\n",
            "            readFiles(file)\n",
            "            count1 += 1\n",
            "            print(f\"{count1} done\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "base",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
